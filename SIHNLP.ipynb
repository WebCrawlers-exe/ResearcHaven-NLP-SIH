{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIHNLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcwouMUKnuEy",
        "outputId": "9054011f-3ac6-407b-8766-ac29b4046c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ahpy in /usr/local/lib/python3.7/dist-packages (2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ahpy) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ahpy) (1.7.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install ahpy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import ahpy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import RegexpStemmer\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "input_file = '/content/sample_data/input_file.txt'\n",
        "output_file = '/content/sample_data/output_file.txt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform preprocessing on input data to get cleaned data\n",
        "def preprocess(input_file):\n",
        "    # perform segmentation\n",
        "    sentences = sent_tokenize(input_file)\n",
        "    # perform tokenization\n",
        "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "    # perform stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_sentences = [[word for word in sentence if word not in stop_words] for sentence in tokenized_sentences]\n",
        "    # perform punctuation removal\n",
        "    punctuations = string.punctuation\n",
        "    filtered_sentences = [[word for word in sentence if word not in punctuations] for sentence in filtered_sentences]\n",
        "    # perform lowercasing\n",
        "    filtered_sentences = [[word.lower() for word in sentence] for sentence in filtered_sentences]\n",
        "    # perform lemmatization and stemming\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmed_sentences = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in filtered_sentences]\n",
        "    # perform pos tagging\n",
        "    pos_tagged_sentences = [nltk.pos_tag(sentence) for sentence in stemmed_sentences]\n",
        "    # perform bag of words\n",
        "    bag_of_words = [ [word for word, pos in sentence] for sentence in pos_tagged_sentences]\n",
        "    # convert bag_of_words to a list of words\n",
        "    bag_of_words = [word for sentence in bag_of_words for word in sentence]\n",
        "    # perform bigram and trigram generation\n",
        "    bigrams = nltk.bigrams(bag_of_words)\n",
        "    trigrams = nltk.trigrams(bag_of_words)\n",
        "\n",
        "\n",
        "    # perform term frequency calculation\n",
        "    term_frequency = {}\n",
        "    for word in bag_of_words:\n",
        "        if word in term_frequency:\n",
        "            term_frequency[word] += 1\n",
        "        else:\n",
        "            term_frequency[word] = 1\n",
        "    \n",
        "    #perform term frequency caluclation for bigrams\n",
        "    bigram_frequency = {}\n",
        "    for bigram in bigrams:\n",
        "        if bigram in bigram_frequency:\n",
        "            bigram_frequency[bigram] += 1\n",
        "        else:\n",
        "            bigram_frequency[bigram] = 1\n",
        "    \n",
        "    #perform term frequency caluclation for trigrams\n",
        "    trigram_frequency = {}\n",
        "    for trigram in trigrams:\n",
        "        if trigram in trigram_frequency:\n",
        "            trigram_frequency[trigram] += 1\n",
        "        else:\n",
        "            trigram_frequency[trigram] = 1\n",
        "\n",
        "    # perform term frequency sorting\n",
        "    sorted_term_frequency = sorted(term_frequency.items(), key=lambda x: x[1], reverse=True)\n",
        "    sorted_bigram_frequency = sorted(bigram_frequency.items(), key=lambda x: x[1], reverse=True)\n",
        "    sorted_trigram_frequency = sorted(trigram_frequency.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # extract words with frequency greater than 10\n",
        "    frequent_words = [word for word, frequency in sorted_term_frequency if frequency > 0]\n",
        "    frequent_bigrams = [bigram for bigram, frequency in sorted_bigram_frequency if frequency > 0]\n",
        "    frequent_trigrams = [trigram for trigram, frequency in sorted_trigram_frequency if frequency > 0]\n",
        "\n",
        "    # generate synonyms for frequent words using wordnet and add them to frequent_words list\n",
        "    syn_words = []\n",
        "    for word in frequent_words:\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for l in syn.lemmas():\n",
        "                syn_words.append(l.name())\n",
        "    frequent_words = frequent_words + syn_words\n",
        "    frequent_words = list(set(frequent_words))\n",
        "\n",
        "    # # generate synonyms for frequent bigrams using wordnet and add them to frequent_bigrams list\n",
        "    # syn_bigrams = []\n",
        "    # for bigram in frequent_bigrams:\n",
        "    #     for syn in wordnet.synsets(bigram[0]):\n",
        "    #         for l in syn.lemmas():\n",
        "    #             syn_bigrams.append((l.name(), bigram[1]))\n",
        "    # frequent_bigrams = frequent_bigrams + syn_bigrams\n",
        "    # frequent_bigrams = list(set(frequent_bigrams))\n",
        "\n",
        "    # # generate synonyms for frequent trigrams using wordnet and add them to frequent_trigrams list\n",
        "    # syn_trigrams = []\n",
        "    # for trigram in frequent_trigrams:\n",
        "    #     for syn in wordnet.synsets(trigram[0]):\n",
        "    #         for l in syn.lemmas():\n",
        "    #             syn_trigrams.append((l.name(), trigram[1]))\n",
        "    # frequent_trigrams = frequent_trigrams + syn_trigrams\n",
        "    # frequent_trigrams = list(set(frequent_trigrams))\n",
        "\n",
        "\n",
        "    return frequent_words, frequent_bigrams, frequent_trigrams"
      ],
      "metadata": {
        "id": "7IHH14cDsLgm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def taking_input(user_file, db_file):\n",
        "    sus_term, sus_bigram, sus_trigram = preprocess(user_file)\n",
        "    db_term, db_bigram, db_trigram = preprocess(db_file)\n",
        "\n",
        "    # return intersection of sus and db\n",
        "    return len(list(set(sus_term).intersection(db_term))), len(list(set(sus_bigram).intersection(db_bigram))), len(list(set(sus_trigram).intersection(db_trigram)))\n",
        "og_title = 'Regularizing Deep Neural Networks with Stochastic Estimators of Hessian Trace '\n",
        "og_abstract = 'In this paper we develop a novel regularization method for deep neural networks by penalizing the trace of Hessian. This regularizer is motivated by a recent guarantee bound of the generalization error. Hutchinson method is a classical unbiased estimator for the trace of a matrix, but it is very time-consuming on deep learning models. Hence a dropout scheme is proposed to efficiently implements the Hutchinson method. Then we discuss a connection to linear stability of a nonlinear dynamical system and flat/sharp minima. Experiments demonstrate that our method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup.'\n",
        "og_introduction = 'Deep neural networks (DNNs) are developing rapidly and are widely used in many fields such as reflection removal [Amanlou et al., 2022], dust pollution [Ebrahimi-Khusfi et al., 2021], building defects detection [Perez et al., 2019], cities and urban development [Nosratabadi et al., 2020]. As more and more models are proposed in the literature, deep neural networks have shown remarkable improvements in performance. However, among various learning problems, over-fitting on training data is a great problem that affects the test accuracy. So a certain regularization method is often needed in the training process. In linear models, Ridge Regression [Hoerl and Kennard, 1970] and Lasso [Tibshirani, 1996] are usually used to avoid over-fitting. They are also called L2 and L1 regularization. L2 regularization has the effect of shrinkage while L1 regularization can be conductive to both shrinkage and sparsity. From the Bayesian perspective, L2 and L1 regularization can be interpreted with normal prior distribution and laplace prior distribution respectively. Apart from L2 and L1 regularization, there are many other forms of regularizers in DNNs. The most widely used one is Weight-Decay [Krogh and Hertz, 1992]. Loshchilov and Hutter [2019] also showed that L2 regularization and Weight-Decay are not identical. Dropout [Srivastava et al., 2014] is another method to avoid over-fitting by reducing co-adapting between units in neural networks. Dropout has inspired a large body of work studying its effects (Wager et al. [2013]; Helmbold and Long [2015]; Wei et al. [2020]). After dropout, various regularization schemes can be applied additionally.'\n",
        "og_keywords = 'AI, Deep Learning, Neural Networks'\n",
        "og_proposed_method = 'We propose a new regularization method for deep neural networks by penalizing the trace of Hessian. This regularizer is motivated by a recent guarantee bound of the generalization error. Hutchinson method is a classical unbiased estimator for the trace of a matrix, but it is very time-consuming on deep learning models. Hence a dropout scheme is proposed to efficiently implements the Hutchinson method. Then we discuss a connection to linear stability of a nonlinear dynamical system and flat/sharp minima. Experiments demonstrate that our method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup.'\n",
        "og_evaluation_result = 'Deep neural networks (DNNs) are developing rapidly and are widely used in many fields such as reflection removal [Amanlou et al., 2022], dust pollution [Ebrahimi-Khusfi et al., 2021], building defects detection [Perez et al., 2019], cities and urban development [Nosratabadi et al., 2020]. As more and more models are proposed in the literature, deep neural networks have shown remarkable improvements in performance. However, among various learning problems, over-fitting on training data is a great problem that affects the test accuracy. So a certain regularization method is often needed in the training process. In linear models, Ridge Regression [Hoerl and Kennard, 1970] and Lasso [Tibshirani, 1996] are usually used to avoid over-fitting. They are also called L2 and L1 regularization. L2 regularization has the effect of shrinkage while L1 regularization can be conductive to both shrinkage and sparsity. From the Bayesian perspective, L2 and L1 regularization can be interpreted with normal prior distribution and laplace prior distribution respectively. Apart from L2 and L1 regularization, there are many other forms of regularizers in DNNs. The most widely used one is Weight-Decay [Krogh and Hertz, 1992]. Loshchilov and Hutter [2019] also showed that L2 regularization and Weight-Decay are not identical. Dropout [Srivastava et al., 2014] is another method to avoid over-fitting by reducing co-adapting between units in neural networks. Dropout has inspired a large body of work studying its effects (Wager et al. [2013]; Helmbold and Long [2015]; Wei et al. [2020]). After dropout, various regularization schemes can be applied additionally.'\n",
        "og_conclusion = 'We found that our proposed method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup. We also found that our proposed method is not identical to L2 and Weight-Decay regularization. We also found that our proposed method is not identical to dropout.'\n",
        "\n",
        "\n",
        "\n",
        "# sus_title = 'Regularizing Deep Neural Networks with Stochastic Estimators of Hessian Trace '\n",
        "# sus_abstract = 'In this paper we develop a novel regularization method for deep neural networks by penalizing the trace of Hessian. This regularizer is motivated by a recent guarantee bound of the generalization error. Hutchinson method is a classical unbiased estimator for the trace of a matrix, but it is very time-consuming on deep learning models. Hence a dropout scheme is proposed to efficiently implements the Hutchinson method. Then we discuss a connection to linear stability of a nonlinear dynamical system and flat/sharp minima. Experiments demonstrate that our method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup.'\n",
        "# sus_introduction = 'Deep neural networks (DNNs) are developing rapidly and are widely used in many fields such as reflection removal [Amanlou et al., 2022], dust pollution [Ebrahimi-Khusfi et al., 2021], building defects detection [Perez et al., 2019], cities and urban development [Nosratabadi et al., 2020]. As more and more models are proposed in the literature, deep neural networks have shown remarkable improvements in performance. However, among various learning problems, over-fitting on training data is a great problem that affects the test accuracy. So a certain regularization method is often needed in the training process. In linear models, Ridge Regression [Hoerl and Kennard, 1970] and Lasso [Tibshirani, 1996] are usually used to avoid over-fitting. They are also called L2 and L1 regularization. L2 regularization has the effect of shrinkage while L1 regularization can be conductive to both shrinkage and sparsity. From the Bayesian perspective, L2 and L1 regularization can be interpreted with normal prior distribution and laplace prior distribution respectively. Apart from L2 and L1 regularization, there are many other forms of regularizers in DNNs. The most widely used one is Weight-Decay [Krogh and Hertz, 1992]. Loshchilov and Hutter [2019] also showed that L2 regularization and Weight-Decay are not identical. Dropout [Srivastava et al., 2014] is another method to avoid over-fitting by reducing co-adapting between units in neural networks. Dropout has inspired a large body of work studying its effects (Wager et al. [2013]; Helmbold and Long [2015]; Wei et al. [2020]). After dropout, various regularization schemes can be applied additionally.'\n",
        "# sus_keywords = 'AI, Deep Learning, Neural Networks'\n",
        "# sus_proposed_method = 'We propose a new regularization method for deep neural networks by penalizing the trace of Hessian. This regularizer is motivated by a recent guarantee bound of the generalization error. Hutchinson method is a classical unbiased estimator for the trace of a matrix, but it is very time-consuming on deep learning models. Hence a dropout scheme is proposed to efficiently implements the Hutchinson method. Then we discuss a connection to linear stability of a nonlinear dynamical system and flat/sharp minima. Experiments demonstrate that our method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup.'\n",
        "# sus_evaluation_result = 'Deep neural networks (DNNs) are developing rapidly and are widely used in many fields such as reflection removal [Amanlou et al., 2022], dust pollution [Ebrahimi-Khusfi et al., 2021], building defects detection [Perez et al., 2019], cities and urban development [Nosratabadi et al., 2020]. As more and more models are proposed in the literature, deep neural networks have shown remarkable improvements in performance. However, among various learning problems, over-fitting on training data is a great problem that affects the test accuracy. So a certain regularization method is often needed in the training process. In linear models, Ridge Regression [Hoerl and Kennard, 1970] and Lasso [Tibshirani, 1996] are usually used to avoid over-fitting. They are also called L2 and L1 regularization. L2 regularization has the effect of shrinkage while L1 regularization can be conductive to both shrinkage and sparsity. From the Bayesian perspective, L2 and L1 regularization can be interpreted with normal prior distribution and laplace prior distribution respectively. Apart from L2 and L1 regularization, there are many other forms of regularizers in DNNs. The most widely used one is Weight-Decay [Krogh and Hertz, 1992]. Loshchilov and Hutter [2019] also showed that L2 regularization and Weight-Decay are not identical. Dropout [Srivastava et al., 2014] is another method to avoid over-fitting by reducing co-adapting between units in neural networks. Dropout has inspired a large body of work studying its effects (Wager et al. [2013]; Helmbold and Long [2015]; Wei et al. [2020]). After dropout, various regularization schemes can be applied additionally.'\n",
        "# sus_conclusion = 'We found that our proposed method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup. We also found that our proposed method is not identical to L2 and Weight-Decay regularization. We also found that our proposed method is not identical to dropout.'\n",
        "\n",
        "\n",
        "# sus_title = 'The Relationship Between Social Media Use and Mental Health in Young Adults'\n",
        "# sus_abstract = 'The present study examined the relationship between social media use and mental health in a sample of young adults. Participants were recruited online and completed measures of social media use, mental health, and demographic information. Results indicated that social media use was significantly associated with mental health, such that higher levels of social media use were associated with worse mental health. These findings suggest that social media use may be a risk factor for mental health problems in young adults.'\n",
        "# sus_introduction= 'The use of social media has become increasingly common in recent years, with nearly two-thirds of American adults reporting that they use at least one social media platform (Duggan & Smith, 2013). Social media use has been linked to a variety of mental health outcomes, including depression, anxiety, and loneliness (e.g., Andreassen, Torsheim, Brunborg, & Pallesen, 2012; Ortega & Reid, 2011). Given the high rates of social media use and the potential for negative mental health effects, it is important to understand the relationship between social media use and mental health in young adults. The present study sought to examine the relationship between social media use and mental health in a sample of young adults. It was hypothesized that social media use would be associated with worse mental health, as measured by self-report scales of depression, anxiety, and loneliness. It was also expected that this relationship would be moderated by gender, such that it would be stronger for women than for men.'\n",
        "# sus_proposed_method = 'Participants were recruited online via social media and other online platforms. Inclusion criteria were that participants be between the ages of 18 and 25 and have used social media in the past month. Exclusion criteria were that participants had a history of mental illness or substance abuse.Participants completed an online survey that included measures of social media use, mental health, and demographic information. The Social Media Use scale (SMU; Andreassen et al., 2012) was used to assess participants’ frequency of social media use, and the Depression, Anxiety, and Stress scale (DASS-21; Lovibond & Lovibond, 1995) was used to assess participants’ symptoms of depression, anxiety, and stress. The UCLA Loneliness Scale (Russell, Peplau, & Ferguson, 1978) was used to assess participants’ feelings of loneliness.'\n",
        "# sus_keywords = 'social media, mental health, young adults, risk factor'\n",
        "# sus_evaluation_result = 'A total of N = 543 participants completed the survey. The mean age was 21.4 years (SD = 2.1), and 55.2% were female. The mean SMU score was 2.9 (SD = 0.8), and the mean DASS-21 score was 9.1 (SD = 7.2). The mean UCLA Loneliness Scale score was 3.5 (SD = 2.5). Pearson’s correlation analyses indicated that social media use was significantly associated with all three mental health outcomes, such that higher levels of social media use were associated with worse mental health (r = .21, p < .01 for depression; r = .22, p < .01 for anxiety; r = .27, p < .01 for loneliness). Hierarchical regression analyses were conducted to examine the moderating effect of gender on the relationship between social media use and mental health. Results indicated that gender did not moderate the relationship between social media use and mental health (β = -.01, p = .68 for depression; β = -.02, p = .54 for anxiety; β = -.03, p = .37 for loneliness).'\n",
        "# sus_conclusion = 'The present study examined the relationship between social media use and mental health in a sample of young adults. Results indicated that social media use was significantly associated with mental health, such that higher levels of social media use were associated with worse mental health. These findings suggest that social media use may be a risk factor for mental health problems in young adults.'\n",
        "# sus_title = 'Deep Neural Network Regularization Using Stochastic Estimators of Hessian Trace'\n",
        "# sus_abstract = 'By penalising the trace of the Hessian, we create a unique regularisation strategy for deep neural networks in this research. A recent guarantee constraint for the generalisation error served as the inspiration for this regularizer. The Hutchinson technique is a traditional unbiased estimator for the trace of a matrix, however deep learning models take a very long time to run. In order to execute the Hutchinson approach effectively, a dropout system is suggested. The relationship between flat/sharp minima and the linear stability of a nonlinear dynamical system is then covered. Studies show that our approach works better than popular regularizers and data augmentation techniques including Jacobian, confidence penalty, label smoothing, cutoff, and mixup.'\n",
        "# sus_introduction = 'Deep neural networks (DNNs) are evolving quickly and are currently being used extensively in a variety of fields, including reflection removal (Amanlou et al., 2022), dust pollution (Ebrahimi-Khusfi et al., 2021), building defect detection (Perez et al., 2019), cities and urban development (Nosratabadi et al., 2020), and more. Deep neural networks have significantly improved in performance as more and more models are put forth in the literature. Overfitting on training data, however, is a significant issue that has an impact on test accuracy among other learning issues. Thus, in the training phase, a particular regularisation technique is frequently required. Ridge Regression [Hoerl and Kennard, 1970] and Lasso [Tibshirani, 1996] are commonly employed in linear models to prevent over-fitting. Additionally known as L2 and L1 regularisation. L2 regularisation has the following impact shrinkage, but both shrinkage and sparsity can be induced via L1 regularisation. L2 and L1 regularisation can be understood from a Bayesian perspective using the normal prior distribution and laplace prior distribution, respectively. There are other regularizers in DNNs besides L2 and L1 regularisation. Weight-Decay is the one that is most frequently employed [Krogh and Hertz, 1992]. Additionally, Loshchilov and Hutter [2019] demonstrated that Weight-Decay and L2 regularisation are not equivalent. Another strategy to prevent over-fitting is dropout [Srivastava et al., 2014], which reduces co-adaptation between neural network units. Large amounts of research have been done on the impacts of dropout (Wager et al. [2013]; Helmbold and Long [2015]; Wei et al. [2020]). After dropping out, alternative regularisation plans may also be used.'\n",
        "# sus_keywords = 'AI, Deep Learning, Neural Networks, Regularization, Dropout, Weight Decay'\n",
        "# sus_proposed_method = 'We offer a new regularisation strategy for deep neural networks that penalises the Hessian trace. A recent guarantee bound of the generalisation error spurred the development of this regularizer. The Hutchinson technique is a traditional unbiased estimator for the trace of a matrix, however it takes a long time on deep learning models. As a result, a dropout strategy is given in order to efficiently apply the Hutchinson approach. The connection between linear stability of a nonlinear dynamical system and flat/sharp minima is then discussed. Experiments show that our method outperforms existing regularizers and data augmentation techniques like Jacobian, confidence penalty, label smoothing, cutoff, and mixup.'\n",
        "# sus_evaluation_result = 'Deep neural networks (DNNs) are evolving quickly and are currently being used extensively in a variety of fields, including reflection removal (Amanlou et al., 2022), dust pollution (Ebrahimi-Khusfi et al., 2021), building defect detection (Perez et al., 2019), cities and urban development (Nosratabadi et al., 2020), and more. Deep neural networks have significantly improved in performance as more and more models are put forth in the literature. Overfitting on training data, however, is a significant issue that has an impact on test accuracy among other learning issues. Thus, in the training phase, a particular regularisation technique is frequently required. Ridge Regression [Hoerl and Kennard, 1970] and Lasso [Tibshirani, 1996] are commonly employed in linear models to prevent over-fitting. Additionally known as L2 and L1 regularisation. L2 regularisation has the following impact shrinkage, but both shrinkage and sparsity can be induced via L1 regularisation. L2 and L1 regularisation can be understood from a Bayesian perspective using the normal prior distribution and laplace prior distribution, respectively. There are other regularizers in DNNs besides L2 and L1 regularisation. Weight-Decay is the one that is most frequently employed [Krogh and Hertz, 1992]. Additionally, Loshchilov and Hutter [2019] demonstrated that Weight-Decay and L2 regularisation are not equivalent. Another strategy to prevent over-fitting is dropout [Srivastava et al., 2014], which reduces co-adaptation between neural network units. Large amounts of research have been done on the impacts of dropout (Wager et al. [2013]; Helmbold and Long [2015]; Wei et al. [2020]). After dropping out, alternative regularisation plans may also be used.'\n",
        "# sus_conclusion = 'Our suggested method outperforms existing regularizers and data augmentation methods like Jacobian, confidence penalty, label smoothing, cutoff, and mixup. We also discovered that our proposed method differs from L2 and Weight-Decay regularisation. We also discovered that our proposed approach is not the same as dropout.'\n",
        "\n",
        "og = [og_title, og_abstract, og_keywords, og_introduction, og_proposed_method, og_evaluation_result, og_conclusion]\n",
        "sus = [sus_title, sus_abstract, sus_keywords, sus_introduction, sus_proposed_method, sus_evaluation_result, sus_conclusion]\n",
        "\n",
        "\n",
        "C = {}\n",
        "paper_parameters = [\"title\", \"abstract\", \"keywords\", \"introduction\", \"proposed_method\", \"evaluation_result\", \"conclusion\"]\n",
        "noOfterms = ['candidate_set', '2termset', '3termset']\n",
        "\n",
        "for i, param in enumerate(paper_parameters):\n",
        "  temp1, temp2, temp3 = taking_input(sus[i],og[i])\n",
        "  print(temp1, temp2 , temp3)\n",
        "  C[param + '_' + noOfterms[0]] = temp1\n",
        "  C[param + '_' + noOfterms[1]] = temp2\n",
        "  C[param + '_' + noOfterms[2]] = temp3\n",
        "\n",
        "\n",
        "\n",
        "# C['abstract'] = taking_input(og_abstract, sus_abstract)\n",
        "# C['intro'] = taking_input(og_intro, sus_intro)\n",
        "# C['title'] = taking_input(og_title, sus_title)\n",
        "# C['keywords'] = taking_input(og_keywords, sus_keywords)\n",
        "# C['proposed_method'] = taking_input(og_proposed_method, sus_proposed_method)\n",
        "# C['evaluation_results'] = taking_input(og_evaluation_results, sus_evaluation_results)\n",
        "# C['conclusion'] = taking_input(og_conclusion, sus_conclusion)\n",
        "\n",
        "\n",
        "pairwise_comparisons ={('title', 'abstract'): 7, ('title', 'keywords'): 7, ('title', 'introduction'): 9, ('title', 'proposed_method'): 8,\n",
        "\t\t\t ('title', 'evaluation_result'): 8, ('title', 'conclusion'): 7,\n",
        "                         ('abstract', 'keywords'): 1, ('abstract', 'introduction'): 5, ('abstract', 'proposed_method'): 3,\n",
        "                         ('abstract', 'evaluation_result'): 3, ('abstract', 'conclusion'): 1,\n",
        "                         ('keywords', 'introduction'): 5, ('keywords', 'proposed_method'): 3, ('keywords', 'evaluation_result'): 3,\n",
        "                         ('keywords', 'conclusion'): 1,\n",
        "                         ('introduction', 'proposed_method'): 0.5, ('introduction', 'evaluation_result'): 0.5, ('introduction', 'conclusion'): 0.33,\n",
        "                         ('proposed_method', 'evaluation_result'): 1, ('proposed_method', 'conclusion'): 0.5,\n",
        "                         ('evaluation_result', 'conclusion'): 0.5}\n",
        "\n",
        "\n",
        "pairwise_comparisons2={('candidate_set', '2termset'): 0.14, ('candidate_set', '3termset'): 0.11, ('2termset', '3termset'):0.14}\n",
        "compare = ahpy.Compare(name='compare',comparisons= pairwise_comparisons, precision=3,random_index='saaty')\n",
        "compare2 = ahpy.Compare(name='compare2',comparisons= pairwise_comparisons2, precision=3,random_index='saaty')\n",
        "\n",
        "weights = compare.target_weights\n",
        "weights2 = compare2.target_weights\n",
        "weights2 = dict(reversed(list(weights2.items())))\n",
        "print(weights2)\n",
        "similarity_score = 0\n",
        "sus = sus_title + sus_abstract + sus_keywords + sus_introduction + sus_proposed_method + sus_evaluation_results + sus_conclusion\n",
        "sus_words = sus.split()\n",
        "suslen = len(sus_words)\n",
        "for i in weights.keys():\n",
        "    for j in weights2.keys():\n",
        "        print(\"w: \",weights[i])\n",
        "        print(\"w2: \",weights2[j])\n",
        "        print(\"C: \",C[i+'_'+j])\n",
        "        similarity_score += weights[i] * weights2[j] * C[i + '_' + j]\n",
        "wk = sum(weights.values()) + sum(weights2.values())\n",
        "print(wk)\n",
        "print(similarity_score)\n",
        "print(suslen)\n",
        "similarity_score = similarity_score / (0.05*suslen)\n",
        "print(similarity_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SfJ9ndsJ2zP",
        "outputId": "6f60c23e-5a9a-4c89-f313-d8d42d67bb65"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61 7 6\n",
            "403 63 63\n",
            "64 4 3\n",
            "884 158 171\n",
            "372 62 62\n",
            "884 158 171\n",
            "116 25 26\n",
            "{'candidate_set': 0.047, '2termset': 0.19, '3termset': 0.763}\n",
            "w:  0.541\n",
            "w2:  0.047\n",
            "C:  61\n",
            "w:  0.541\n",
            "w2:  0.19\n",
            "C:  7\n",
            "w:  0.541\n",
            "w2:  0.763\n",
            "C:  6\n",
            "w:  0.118\n",
            "w2:  0.047\n",
            "C:  403\n",
            "w:  0.118\n",
            "w2:  0.19\n",
            "C:  63\n",
            "w:  0.118\n",
            "w2:  0.763\n",
            "C:  63\n",
            "w:  0.118\n",
            "w2:  0.047\n",
            "C:  64\n",
            "w:  0.118\n",
            "w2:  0.19\n",
            "C:  4\n",
            "w:  0.118\n",
            "w2:  0.763\n",
            "C:  3\n",
            "w:  0.096\n",
            "w2:  0.047\n",
            "C:  116\n",
            "w:  0.096\n",
            "w2:  0.19\n",
            "C:  25\n",
            "w:  0.096\n",
            "w2:  0.763\n",
            "C:  26\n",
            "w:  0.049\n",
            "w2:  0.047\n",
            "C:  372\n",
            "w:  0.049\n",
            "w2:  0.19\n",
            "C:  62\n",
            "w:  0.049\n",
            "w2:  0.763\n",
            "C:  62\n",
            "w:  0.049\n",
            "w2:  0.047\n",
            "C:  884\n",
            "w:  0.049\n",
            "w2:  0.19\n",
            "C:  158\n",
            "w:  0.049\n",
            "w2:  0.763\n",
            "C:  171\n",
            "w:  0.03\n",
            "w2:  0.047\n",
            "C:  884\n",
            "w:  0.03\n",
            "w2:  0.19\n",
            "C:  158\n",
            "w:  0.03\n",
            "w2:  0.763\n",
            "C:  171\n",
            "2.0010000000000003\n",
            "37.37864999999999\n",
            "770\n",
            "0.9708740259740258\n"
          ]
        }
      ]
    }
  ]
}